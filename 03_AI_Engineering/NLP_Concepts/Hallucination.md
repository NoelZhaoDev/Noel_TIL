---
title: The hallucination
aliases:
  - å¹»è§‰
date: 2026-01-02
tags:
  - LLM
status:
  - ğŸŒ±
---

# ğŸ¯ Problem / Context 

> *while running **inference** on the **4-bit quantized Mistral-7B-Instruct** with prompt"If the Company notifies the Vendor that it has failed to deliver the Goods, it shall verify the status.Question: Who is the second 'it' referring to?", I observed that the model produced **nonsensical responses** five trials, which I identified as a clear case of **hallucination***

# âœ… Solution / Concept 

*   **Core Concept:** hallucination
*   **Explanation:**
    *   æ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°æ¦‚ç‡åˆ†å¸ƒè€Œèƒ¡ä¹±ç”Ÿæˆçš„å†…å®¹ã€‚
    *   å°è¯•åœ¨è¾“å…¥å±‚æ³¨å…¥è¯­è¨€å­¦è§„åˆ™çº¦æŸåŠ ä»¥è§£å†³ã€‚

# ğŸ’» Code Snippet 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "Mistral-7B-Instruct-v0.2"

bnb_config = BitsAndBytesConfig(

 Â  Â load_in_4bit=True,

 Â  Â bnb_4bit_compute_dtype=torch.float16,

 Â  Â bnb_4bit_use_double_quant=True,

 Â  Â bnb_4bit_quant_type="nf4", Â # å…³é”®ï¼šNF4

)

tokenizer = AutoTokenizer.from_pretrained(

Â  Â  model_id,

Â  Â  use_fast=True

)

model = AutoModelForCausalLM.from_pretrained(

Â  Â  model_id,

Â  Â  quantization_config=bnb_config,

Â  Â  device_map="auto",

Â  Â  torch_dtype=torch.float16,

)

def llm_fallback(prompt, max_new_tokens=128):

Â  Â  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

Â  Â  with torch.no_grad():

Â  Â  Â  Â  outputs = model.generate(

Â  Â  Â  Â  Â  Â  **inputs,

Â  Â  Â  Â  Â  Â  max_new_tokens=max_new_tokens,

Â  Â  Â  Â  Â  Â  do_sample=True, Â 

Â  Â  Â  Â  Â  Â  temperature=0.7,

Â  Â  Â  Â  )

Â  Â  return tokenizer.decode(outputs[0], skip_special_tokens=True)
Â  Â  
text= "If the Company notifies the Vendor that it has failed to deliver the Goods, it shall verify the status.Question: Who is the second 'it' referring to?"

test = llm_fallback(text)

test

```

